{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import easyocr\n",
    "\n",
    "\n",
    "# Path to images\n",
    "IMAGE_DIR = \"C:/Users/nazih/OneDrive/Desktop/Decoding-Audio-Captchas/images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text: ['idGTid']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize EasyOCR reader (for English language, you can add other languages if needed)\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "# Path to the image you want to process\n",
    "image_path =\"C:/Users/nazih/Desktop/Decoding-Audio-Captchas/images/captcha_9984.png\"\n",
    "# Extract text from the image\n",
    "result = reader.readtext(image_path, detail=0)  # detail=0 returns just the text\n",
    "\n",
    "# Print extracted text\n",
    "print(\"Extracted Text:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved at: extracted_texts.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize EasyOCR reader (for English language)\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "# Path to the folder containing your images\n",
    "image_folder = 'C:/Users/nazih/Desktop/Decoding-Audio-Captchas/images'\n",
    "\n",
    "# List all image files in the folder (you can filter by image extensions like .png, .jpg)\n",
    "image_paths = [os.path.join(image_folder, filename) for filename in os.listdir(image_folder) if filename.endswith(('.png'))]\n",
    "\n",
    "# Extract text from each image and store in a list\n",
    "extracted_texts = []\n",
    "\n",
    "for image_path in image_paths:\n",
    "    result = reader.readtext(image_path, detail=0)  # detail=0 returns just the text\n",
    "    extracted_texts.append({'image_path': image_path, 'extracted_text': result[0] if result else ''})\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(extracted_texts)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "csv_file_path = 'extracted_texts.csv'\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"CSV file saved at: {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file with audio file names and labels saved at: audio_to_text_mapping.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Path to the image and audio files\n",
    "image_folder = 'C:/Users/nazih/Desktop/Decoding-Audio-Captchas/images'\n",
    "audio_folder = 'C:/Users/nazih/Desktop/Decoding-Audio-Captchas/audio'\n",
    "\n",
    "# Read the CSV containing the extracted text from images\n",
    "extracted_df = pd.read_csv('extracted_texts.csv')\n",
    "\n",
    "# List all audio files in the audio folder (assuming they have the same name as image files)\n",
    "audio_files = [f for f in os.listdir(audio_folder) if f.endswith(('.wav', '.mp3'))]\n",
    "\n",
    "# Create a mapping for audio files to labels (assuming audio and image files share the same names)\n",
    "audio_to_text_mapping = []\n",
    "\n",
    "for image_file in extracted_df['image_path']:\n",
    "    # Extract the file name (without extension) to match audio files\n",
    "    base_name = os.path.basename(image_file).split('.')[0]  # Get the name without the extension\n",
    "    audio_file = f\"{base_name}.wav\"  # Adjust this if audio file format is different (e.g., .mp3)\n",
    "\n",
    "    # Check if the corresponding audio file exists\n",
    "    if audio_file in audio_files:\n",
    "        label = extracted_df[extracted_df['image_path'] == image_file]['extracted_text'].values[0]\n",
    "        audio_to_text_mapping.append({'audio_filename': audio_file, 'extracted_text': label})\n",
    "\n",
    "# Create a DataFrame from the mapping\n",
    "mapping_df = pd.DataFrame(audio_to_text_mapping)\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "csv_file_path = 'audio_to_text_mapping.csv'\n",
    "mapping_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"CSV file with audio file names and labels saved at: {csv_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     audio_filename extracted_text\n",
      "0  captcha_0001.wav         ABOr4z\n",
      "1  captcha_0002.wav         XLMbrT\n",
      "2  captcha_0003.wav         bBRG5o\n",
      "3  captcha_0004.wav         fdzhYy\n",
      "4  captcha_0005.wav         SvOUfs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load dataset\n",
    "dataset_path = \"audio_to_text_mapping.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Print dataset preview\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: en\n",
      "Transcription text: capital C, small G, small D, 8, small A, small R.\n",
      "Processed text: Cgd8ar\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import re\n",
    "\n",
    "# Load Whisper model\n",
    "model = whisper.load_model('base')\n",
    "\n",
    "# Load and process the audio\n",
    "audio = whisper.load_audio(\"C:/Users/nazih/Desktop/Decoding-Audio-Captchas/audio/captcha_6000.wav\")\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# Create Mel spectrogram and detect language\n",
    "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "# Perform the transcription\n",
    "options = whisper.DecodingOptions()\n",
    "result = whisper.decode(model, mel, options)\n",
    "\n",
    "# Print the detected language\n",
    "print(f\"Detected Language: {result.language}\")\n",
    "\n",
    "# Access the best transcription (most likely transcription)\n",
    "print(\"Transcription text:\", result.text)\n",
    "\n",
    "# If you want alternatives (if show_all=True was enabled)\n",
    "if hasattr(result, 'segments'):\n",
    "    for segment in result.segments:\n",
    "        print(f\"Alternative: {segment['text']} with confidence: {segment['confidence']}\")\n",
    "\n",
    "# Process the result text to handle 'small' and 'capital' modifiers\n",
    "def process_text(text):\n",
    "    words = text.split()\n",
    "    processed_text = \"\"\n",
    "    state = 'normal'  # Start with 'normal' state\n",
    "\n",
    "    for word in words:\n",
    "        if state == 'normal':\n",
    "            if word.lower() == 'small':\n",
    "                state = 'expecting_lower'\n",
    "            elif word.lower() == 'capital':\n",
    "                state = 'expecting_upper'\n",
    "            else:\n",
    "                processed_text += word\n",
    "        elif state == 'expecting_lower':\n",
    "            processed_text += word.lower()\n",
    "            state = 'normal'\n",
    "        elif state == 'expecting_upper':\n",
    "            processed_text += word.upper()\n",
    "            state = 'normal'\n",
    "    processed_text = processed_text.replace(\",\", \"\").strip()\n",
    "    processed_text = processed_text.replace(\".\", \"\").strip()\n",
    "    return processed_text\n",
    "\n",
    "# Process and correct the transcription\n",
    "processed_text =process_text(result.text)\n",
    "print(\"Processed text:\", processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABOr4z BBOr4z\n",
      "CER for captcha_0001.wav: 16.67%\n",
      "Accuracy for captcha_0001.wav: 0.00%\n",
      "xLMbrT xLMbrT\n",
      "CER for captcha_0002.wav: 0.00%\n",
      "Accuracy for captcha_0002.wav: 100.00%\n",
      "bBRG5o bBRGFineZero\n",
      "CER for captcha_0003.wav: 116.67%\n",
      "Accuracy for captcha_0003.wav: 0.00%\n",
      "fd2hYy fb2hYy\n",
      "CER for captcha_0004.wav: 16.67%\n",
      "Accuracy for captcha_0004.wav: 0.00%\n",
      "SvOUfs andv0Ufs\n",
      "CER for captcha_0005.wav: 66.67%\n",
      "Accuracy for captcha_0005.wav: 0.00%\n",
      "86dHWG 8dEDGEWG\n",
      "CER for captcha_0006.wav: 83.33%\n",
      "Accuracy for captcha_0006.wav: 0.00%\n",
      "hHrZVn edgeHr7Vn\n",
      "CER for captcha_0007.wav: 83.33%\n",
      "Accuracy for captcha_0007.wav: 0.00%\n",
      "hmRfZv hmRfZv\n",
      "CER for captcha_0008.wav: 0.00%\n",
      "Accuracy for captcha_0008.wav: 100.00%\n",
      "wYDOet wYDZimroat\n",
      "CER for captcha_0009.wav: 100.00%\n",
      "Accuracy for captcha_0009.wav: 0.00%\n",
      "7gIAHO gIAEDGEL\n",
      "CER for captcha_0010.wav: 100.00%\n",
      "Accuracy for captcha_0010.wav: 0.00%\n",
      "vutDgy vutDgY\n",
      "CER for captcha_0011.wav: 16.67%\n",
      "Accuracy for captcha_0011.wav: 0.00%\n",
      "jJdijT jJdijT\n",
      "CER for captcha_0012.wav: 0.00%\n",
      "Accuracy for captcha_0012.wav: 100.00%\n",
      "Bcclyo BccIyo\n",
      "CER for captcha_0013.wav: 16.67%\n",
      "Accuracy for captcha_0013.wav: 0.00%\n",
      "rtlILc rtlILc\n",
      "CER for captcha_0014.wav: 0.00%\n",
      "Accuracy for captcha_0014.wav: 100.00%\n",
      "25gLRC 25gLRC\n",
      "CER for captcha_0015.wav: 0.00%\n",
      "Accuracy for captcha_0015.wav: 100.00%\n",
      "Rpqomo RpqZimromgo\n",
      "CER for captcha_0016.wav: 83.33%\n",
      "Accuracy for captcha_0016.wav: 0.00%\n",
      "IN8vmv IN8vmv\n",
      "CER for captcha_0017.wav: 0.00%\n",
      "Accuracy for captcha_0017.wav: 100.00%\n",
      "jQEIbm jQAlbm\n",
      "CER for captcha_0018.wav: 33.33%\n",
      "Accuracy for captcha_0018.wav: 0.00%\n",
      "4BMsoq forBMsoq\n",
      "CER for captcha_0019.wav: 50.00%\n",
      "Accuracy for captcha_0019.wav: 0.00%\n",
      "OBMJ9r boBMJ9r\n",
      "CER for captcha_0020.wav: 33.33%\n",
      "Accuracy for captcha_0020.wav: 0.00%\n",
      "KovZug K6vZug\n",
      "CER for captcha_0021.wav: 16.67%\n",
      "Accuracy for captcha_0021.wav: 0.00%\n",
      "LROhAO LR0hA0\n",
      "CER for captcha_0022.wav: 33.33%\n",
      "Accuracy for captcha_0022.wav: 0.00%\n",
      "JDZwO7 JD2wOCivil\n",
      "CER for captcha_0023.wav: 100.00%\n",
      "Accuracy for captcha_0023.wav: 0.00%\n",
      "Zfgnis Zfqnis\n",
      "CER for captcha_0024.wav: 16.67%\n",
      "Accuracy for captcha_0024.wav: 0.00%\n",
      "TxFg90 7xFg9o\n",
      "CER for captcha_0025.wav: 33.33%\n",
      "Accuracy for captcha_0025.wav: 0.00%\n",
      "q4eSJg qaSJg\n",
      "CER for captcha_0026.wav: 33.33%\n",
      "Accuracy for captcha_0026.wav: 0.00%\n",
      "z6xi6H z6xi6H\n",
      "CER for captcha_0027.wav: 0.00%\n",
      "Accuracy for captcha_0027.wav: 100.00%\n",
      "wTpAIB wTpAlB\n",
      "CER for captcha_0028.wav: 16.67%\n",
      "Accuracy for captcha_0028.wav: 0.00%\n",
      "UOZSZP U0ZSZP\n",
      "CER for captcha_0029.wav: 16.67%\n",
      "Accuracy for captcha_0029.wav: 0.00%\n",
      "DSjK8E DSjKHeyA\n",
      "CER for captcha_0030.wav: 66.67%\n",
      "Accuracy for captcha_0030.wav: 0.00%\n",
      "AAHTWT HTWT\n",
      "CER for captcha_0031.wav: 33.33%\n",
      "Accuracy for captcha_0031.wav: 0.00%\n",
      "IGZbPy IGZbPy\n",
      "CER for captcha_0032.wav: 0.00%\n",
      "Accuracy for captcha_0032.wav: 100.00%\n",
      "kQAct5 kQACt5\n",
      "CER for captcha_0033.wav: 16.67%\n",
      "Accuracy for captcha_0033.wav: 0.00%\n",
      "NVplmo MVplmO\n",
      "CER for captcha_0034.wav: 33.33%\n",
      "Accuracy for captcha_0034.wav: 0.00%\n",
      "Ugzacc 9zSmallercc\n",
      "CER for captcha_0035.wav: 133.33%\n",
      "Accuracy for captcha_0035.wav: 0.00%\n",
      "x8tjaE xatjsmallerA\n",
      "CER for captcha_0036.wav: 133.33%\n",
      "Accuracy for captcha_0036.wav: 0.00%\n",
      "tZHDvj t7HDvj\n",
      "CER for captcha_0037.wav: 16.67%\n",
      "Accuracy for captcha_0037.wav: 0.00%\n",
      "XSXsEO xSxsEzero\n",
      "CER for captcha_0038.wav: 100.00%\n",
      "Accuracy for captcha_0038.wav: 0.00%\n",
      "5057nc i'mOi'm7nC\n",
      "CER for captcha_0039.wav: 133.33%\n",
      "Accuracy for captcha_0039.wav: 0.00%\n",
      "D843gi DHey43gi\n",
      "CER for captcha_0040.wav: 50.00%\n",
      "Accuracy for captcha_0040.wav: 0.00%\n",
      "iMosE9 iMosE9\n",
      "CER for captcha_0041.wav: 0.00%\n",
      "Accuracy for captcha_0041.wav: 100.00%\n",
      "axV9Zu SmallerxV9Zu\n",
      "CER for captcha_0042.wav: 100.00%\n",
      "Accuracy for captcha_0042.wav: 0.00%\n",
      "pctUXg pctUXg\n",
      "CER for captcha_0043.wav: 0.00%\n",
      "Accuracy for captcha_0043.wav: 100.00%\n",
      "OLYA9D SmallboeLYA9D\n",
      "CER for captcha_0044.wav: 133.33%\n",
      "Accuracy for captcha_0044.wav: 0.00%\n",
      "rxRNZQ rxRN2Q\n",
      "CER for captcha_0045.wav: 16.67%\n",
      "Accuracy for captcha_0045.wav: 0.00%\n",
      "HCPfpd HCPfpd\n",
      "CER for captcha_0046.wav: 0.00%\n",
      "Accuracy for captcha_0046.wav: 100.00%\n",
      "hRTbgx hRTbgx\n",
      "CER for captcha_0047.wav: 0.00%\n",
      "Accuracy for captcha_0047.wav: 100.00%\n",
      "jvTZMN jvTZMN\n",
      "CER for captcha_0048.wav: 0.00%\n",
      "Accuracy for captcha_0048.wav: 100.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 100\u001b[0m\n\u001b[0;32m     97\u001b[0m csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/nazih/Desktop/Decoding-Audio-Captchas/audio_to_text_mapping.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Run the transcription and evaluation\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m transcriptions \u001b[38;5;241m=\u001b[39m \u001b[43mtranscribe_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 71\u001b[0m, in \u001b[0;36mtranscribe_and_evaluate\u001b[1;34m(audio_directory, csv_file)\u001b[0m\n\u001b[0;32m     68\u001b[0m audio \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mpad_or_trim(audio)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Perform the transcription\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m processed_text \u001b[38;5;241m=\u001b[39m process_text(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Get the correct transcription from CSV\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python\\Python311\\Lib\\site-packages\\whisper\\transcribe.py:295\u001b[0m, in \u001b[0;36mtranscribe\u001b[1;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, carry_initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    293\u001b[0m     decode_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_tokens[prompt_reset_since:]\n\u001b[1;32m--> 295\u001b[0m result: DecodingResult \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel_segment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(result\u001b[38;5;241m.\u001b[39mtokens)\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_speech_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# no voice activity check\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python\\Python311\\Lib\\site-packages\\whisper\\transcribe.py:201\u001b[0m, in \u001b[0;36mtranscribe.<locals>.decode_with_fallback\u001b[1;34m(segment)\u001b[0m\n\u001b[0;32m    198\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_of\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    200\u001b[0m options \u001b[38;5;241m=\u001b[39m DecodingOptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, temperature\u001b[38;5;241m=\u001b[39mt)\n\u001b[1;32m--> 201\u001b[0m decode_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m needs_fallback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    205\u001b[0m     compression_ratio_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m decode_result\u001b[38;5;241m.\u001b[39mcompression_ratio \u001b[38;5;241m>\u001b[39m compression_ratio_threshold\n\u001b[0;32m    207\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python\\Python311\\Lib\\site-packages\\whisper\\decoding.py:824\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(model, mel, options, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m    822\u001b[0m     options \u001b[38;5;241m=\u001b[39m replace(options, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 824\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mDecodingTask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m single \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python\\Python311\\Lib\\site-packages\\whisper\\decoding.py:718\u001b[0m, in \u001b[0;36mDecodingTask.run\u001b[1;34m(self, mel)\u001b[0m\n\u001b[0;32m    715\u001b[0m tokenizer: Tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\n\u001b[0;32m    716\u001b[0m n_audio: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m mel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 718\u001b[0m audio_features: Tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_audio_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# encoder forward pass\u001b[39;00m\n\u001b[0;32m    719\u001b[0m tokens: Tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_tokens])\u001b[38;5;241m.\u001b[39mrepeat(n_audio, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    721\u001b[0m \u001b[38;5;66;03m# detect language if requested, overwriting the language token\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python\\Python311\\Lib\\site-packages\\whisper\\decoding.py:655\u001b[0m, in \u001b[0;36mDecodingTask._get_audio_features\u001b[1;34m(self, mel)\u001b[0m\n\u001b[0;32m    653\u001b[0m     audio_features \u001b[38;5;241m=\u001b[39m mel\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 655\u001b[0m     audio_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m audio_features\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m (\n\u001b[0;32m    658\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfloat16 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m    659\u001b[0m ):\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_features has an incorrect dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio_features\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    662\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Python\\Python311\\Lib\\site-packages\\whisper\\model.py:201\u001b[0m, in \u001b[0;36mAudioEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    198\u001b[0m x \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m--> 201\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_post(x)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Python\\Python311\\Lib\\site-packages\\whisper\\model.py:170\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[1;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn:\n\u001b[0;32m    169\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn_ln(x), xa, kv_cache\u001b[38;5;241m=\u001b[39mkv_cache)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 170\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_ln\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Python\\Python311\\Lib\\site-packages\\whisper\\model.py:46\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import os\n",
    "import pandas as pd\n",
    "import editdistance\n",
    "\n",
    "# Function to process the text from Whisper's output\n",
    "def process_text(text):\n",
    "    words = text.split()\n",
    "    processed_text = \"\"\n",
    "    state = 'normal'  # Start with 'normal' state\n",
    "\n",
    "    for word in words:\n",
    "        if state == 'normal':\n",
    "            if word.lower() == 'small':\n",
    "                state = 'expecting_lower'\n",
    "            elif word.lower() == 'capital':\n",
    "                state = 'expecting_upper'\n",
    "            else:\n",
    "                processed_text += word\n",
    "        elif state == 'expecting_lower':\n",
    "            processed_text += word.lower()\n",
    "            state = 'normal'\n",
    "        elif state == 'expecting_upper':\n",
    "            processed_text += word.upper()\n",
    "            state = 'normal'\n",
    "\n",
    "    return processed_text.replace(\",\", \"\").replace(\".\", \"\").strip()\n",
    "\n",
    "# Function to load the correct texts from CSV\n",
    "def load_correct_texts(csv_file):\n",
    "    # Load the CSV into a pandas dataframe\n",
    "    df = pd.read_csv(csv_file)\n",
    "    return {row['audio_filename']: row['extracted_text'] for _, row in df.iterrows()}\n",
    "\n",
    "# Function to calculate character error rate (CER)\n",
    "def calculate_cer(reference, hypothesis):\n",
    "    reference = reference.replace(\" \", \"\")  # Remove spaces for CER calculation\n",
    "    hypothesis = hypothesis.replace(\" \", \"\")  # Remove spaces for CER calculation\n",
    "    return editdistance.eval(reference, hypothesis) / len(reference)\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(reference, hypothesis):\n",
    "    reference_words = reference.split()\n",
    "    hypothesis_words = hypothesis.split()\n",
    "    correct = sum([1 for r, h in zip(reference_words, hypothesis_words) if r == h])\n",
    "    return correct / len(reference_words)\n",
    "\n",
    "# Transcription and comparison with the correct texts\n",
    "def transcribe_and_evaluate(audio_directory, csv_file):\n",
    "    # Load the Whisper model\n",
    "    model = whisper.load_model(\"base\")\n",
    "    \n",
    "    # Load the correct texts from CSV\n",
    "    correct_texts = load_correct_texts(csv_file)\n",
    "\n",
    "    # List all files in the audio directory\n",
    "    files = [f for f in os.listdir(audio_directory) if f.endswith('.wav')]\n",
    "    \n",
    "    transcribed_texts = []\n",
    "    correct_texts_list = []\n",
    "    \n",
    "    # Process each file\n",
    "    for filename in files:\n",
    "        file_path = os.path.join(audio_directory, filename)\n",
    "        \n",
    "        # Preprocess the audio\n",
    "        audio = whisper.load_audio(file_path)\n",
    "        audio = whisper.pad_or_trim(audio)\n",
    "        \n",
    "        # Perform the transcription\n",
    "        result = model.transcribe(audio, fp16=False)\n",
    "        processed_text = process_text(result['text'])\n",
    "        \n",
    "        # Get the correct transcription from CSV\n",
    "        correct_text = correct_texts.get(filename, None)\n",
    "        \n",
    "        if correct_text:\n",
    "            # Add to the list of transcribed and correct texts for CER and accuracy calculation\n",
    "            transcribed_texts.append(processed_text)\n",
    "            correct_texts_list.append(correct_text)\n",
    "            \n",
    "            # Calculate and print CER for each audio file\n",
    "            cer = calculate_cer(correct_text, processed_text)\n",
    "            accuracy = calculate_accuracy(correct_text, processed_text)\n",
    "            print(correct_text, processed_text)\n",
    "            print(f\"CER for {filename}: {cer * 100:.2f}%\")\n",
    "            print(f\"Accuracy for {filename}: {accuracy * 100:.2f}%\")\n",
    "        else:\n",
    "            print(f\"Warning: Correct text for {filename} not found in CSV.\")\n",
    "    \n",
    "    return transcribed_texts\n",
    "\n",
    "# Path to the directory containing your audio files\n",
    "audio_directory = \"C:/Users/nazih/Desktop/Decoding-Audio-Captchas/audio\"\n",
    "\n",
    "# Path to your CSV file containing correct texts\n",
    "csv_file = \"C:/Users/nazih/Desktop/Decoding-Audio-Captchas/audio_to_text_mapping.csv\"\n",
    "\n",
    "# Run the transcription and evaluation\n",
    "transcriptions = transcribe_and_evaluate(audio_directory, csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
